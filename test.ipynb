{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# vezme se bottleneck vektor a vynásobí se query maticí a dostaneme query vektor, to se dělá zvlášť (ne paralelně)\n",
    "# pro každý token vypočítat jeho K a V (pro každý modul jiný) - každý modul má jinou K a V matici\n",
    "# 1 matice, kterou když splitnu dostanu K a V - jako u c_attn(), jen to musíme paralelizovat na N modulů\n",
    "# tahle matice bude tensor o N dimenzích, 1 dimenze = 1 modul\n",
    "# 4x16 sloupců budou embeddingy tokenů, musí mít stejnou hloubku jako tensor a budou se násobit 1:1, tím získám K V vektory pro každý modul\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 64\n",
    "num_tokens = 4\n",
    "num_modules = 16\n",
    "dim_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 32])\n",
      "torch.Size([64, 32, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "in_t = torch.rand(batch, num_tokens*num_modules, dim_size)\n",
    "print(in_t.shape)\n",
    "\n",
    "in_t = in_t.reshape(batch, dim_size, -1, num_modules) # 4 počet tokenů pro modul, 32 dimenze embeddingů, 16 počet modulů\n",
    "print(in_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.rand(2*dim_size, dim_size, num_modules) # 16 modulů, 2*32 - 2 embedding vektory o velikosti 32, 32 embedding vstupu\n",
    "m.shape\n",
    "\n",
    "# tensor A který má velikost 4*2*3 (4 - řádky, 2 - sloupce, 3 - matice)\n",
    "# s tensorem B který má velikost 2*2*3 (2 - řádky, 2 - sloupce, 3 - matice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_param = torch.nn.Parameter(m)\n",
    "m = m_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "result = torch.einsum('ijk,bjlk->bilk', m, in_t)\n",
    "print(result.shape)  # Should be (64, 4, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vypočítat query vektor (bottleneck vynásobíme query maticí 32*32) - bottleneck náhodný vektor o dimenzi 32, query matice taky náhodná\n",
    "# potom vypočítat attention - skalární součin mezi každým key vektorem (první půlka řádků v každé matici) a query maticí\n",
    "# tím získám logity, na to softmax\n",
    "# 2 varianty - softmax v rámci každého modulu, agregace v rámci modulu těch value vektorů\n",
    "# softmax se vypočítá přes všechno najednou, v rámci všech modulů"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck = torch.rand(batch, 32)\n",
    "q_matrix = torch.rand(batch, 32, 32)\n",
    "\n",
    "q_vec = torch.bmm(bottleneck.unsqueeze(1), q_matrix).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_vectors = result[:, :dim_size, :, :]\n",
    "att = torch.einsum(\"bijk,bi->bjk\", key_vectors, q_vec)\n",
    "#attention_scores = torch.sum(key_vectors * q_vec_reshaped, dim=1)\n",
    "#attention_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 4, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (32, 16)\n",
    "# flattened_scores = att.reshape(-1)\n",
    "import torch.nn.functional as F\n",
    "attention_probs = F.softmax(att.reshape(batch, -1), dim=1)\n",
    "\n",
    "value_vectors = result[:, dim_size:, :, :].reshape(batch, dim_size, -1)  # (32,64)\n",
    "\n",
    "aggregated_values = torch.einsum('bij,bj->bi', value_vectors, attention_probs)  # (32, 16)\n",
    "\n",
    "aggregated_values.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
